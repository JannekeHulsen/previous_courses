{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71efc0e9",
   "metadata": {},
   "source": [
    "## Course: Deep Learning\n",
    "### Date: Nov-Dec 2023\n",
    "\n",
    "Here, we were tasked to create a neural network for MNIST data. The network consists of two linear layers, a hidden layer with 300 nodes, a sigmoid activation, an output layer with 10 classes, and a softmax activation over the output layer.\n",
    "<br><br>\n",
    "The functions `load()` and `load_mnist()` were provided. Only plain python and the `numpy` package could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "def load_mnist(final=False, flatten=True):\n",
    "    \"\"\"\n",
    "    Load the MNIST data.\n",
    "\n",
    "    :param final: If true, return the canonical test/train split. If false, split some validation data from the training\n",
    "       data and keep the test data hidden.\n",
    "    :param flatten: If true, each instance is flattened into a vector, so that the data is returns as a matrix with 768\n",
    "        columns. If false, the data is returned as a 3-tensor preserving each image as a matrix.\n",
    "\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data and the corresponding classification labels as a numpy integer array. The second contains the test/validation\n",
    "     data in the same format. The last integer contains the number of classes (this is always 2 for this function).\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    if not os.path.isfile('mnist.pkl'):\n",
    "        init()\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load()\n",
    "    xtl, xsl = xtrain.shape[0], xtest.shape[0]\n",
    "\n",
    "    if flatten:\n",
    "        xtrain = xtrain.reshape(xtl, -1)\n",
    "        xtest  = xtest.reshape(xsl, -1)\n",
    "\n",
    "    if not final: # return the flattened images\n",
    "        return (xtrain[:-5000], ytrain[:-5000]), (xtrain[-5000:], ytrain[-5000:]), 10\n",
    "\n",
    "    return (xtrain, ytrain), (xtest, ytest), 10\n",
    "\n",
    "# Sigmoid activation.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create empty network.\n",
    "def create_empty_network(X):\n",
    "    network = [np.zeros((X.shape[0], 300)), # first layer\n",
    "               np.zeros((X.shape[0], 300)), # sigmoid-activated layer\n",
    "               np.zeros((X.shape[0], 10))] # output layer\n",
    "    return network\n",
    "\n",
    "def forward_pass(X, action_per_layer, weights, biases):\n",
    "    network = create_empty_network(X)\n",
    "    \n",
    "    # Forward pass.\n",
    "    layer = X\n",
    "    passed_network = []\n",
    "    for i in range(len(network)):\n",
    "        if (action_per_layer[i] == \"dense\"):\n",
    "            layer = np.dot(layer, weights[i]) + biases[i]\n",
    "        elif (action_per_layer[i] == \"sigmoid\"):\n",
    "            layer = sigmoid(layer)\n",
    "        passed_network.append(layer)\n",
    "\n",
    "    # Check.\n",
    "    assert len(network) == len(passed_network)\n",
    "    for i in range(len(network)):\n",
    "        assert network[i].shape == passed_network[i].shape\n",
    "        \n",
    "    return passed_network\n",
    "\n",
    "# Get gradient of crossentropy+softmax.\n",
    "# output_layer is the last activated layer.\n",
    "# labels is the list with targets.\n",
    "def get_first_gradient(output_layer, labels):\n",
    "    one_or_zero = np.zeros_like(output_layer)\n",
    "    one_or_zero[np.arange(len(output_layer)), labels] = 1\n",
    "    softmax = do_softmax(output_layer)\n",
    "    gradient = (- one_or_zero + softmax) / output_layer.shape[0]\n",
    "    return gradient\n",
    "\n",
    "# Do backward pass and update gradients.\n",
    "def backward_pass(passed_network, X, labels, action_per_layer, weights, biases, learning_rate = 0.001):\n",
    "    \n",
    "    network = create_empty_network(X)\n",
    "    \n",
    "    # Backpropagation\n",
    "    data_and_layers = [X] + passed_network\n",
    "    gradient = get_first_gradient(passed_network[-1], labels)\n",
    "\n",
    "    for i in range(len(network))[::-1]:\n",
    "        if (action_per_layer[i] == \"dense\"):\n",
    "            # Update weights.\n",
    "            gradients_weights = np.dot(data_and_layers[i].T, gradient)\n",
    "            weights[i] = weights[i] - learning_rate * gradients_weights\n",
    "            # Update biases.\n",
    "            gradients_biases = data_and_layers[i].shape[0] * gradient.mean(axis = 0)\n",
    "            biases[i] = biases[i] - learning_rate * gradients_biases\n",
    "            # Backpropagate gradient.\n",
    "            gradient = np.dot(gradient, weights[i].T)\n",
    "        if (action_per_layer[i] == \"sigmoid\"):\n",
    "            gradient = gradient * data_and_layers[i+1] * (1-data_and_layers[i+1])\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# Apply softmax\n",
    "def do_softmax(output_layer):\n",
    "    softmax_layer = np.zeros_like(output_layer)\n",
    "    for instance_index in range(output_layer.shape[0]):\n",
    "        for class_index in range(output_layer.shape[1]):\n",
    "            softmax = np.exp(output_layer[instance_index][class_index])/np.sum(np.exp(output_layer[instance_index]))\n",
    "            softmax_layer[instance_index][class_index] = softmax\n",
    "    return(softmax_layer)\n",
    "\n",
    "# Accuracy.\n",
    "def compute_accuracy(softmax_layer, labels):\n",
    "    accuracy = np.mean(np.argmax(softmax_layer, axis=1) == labels)\n",
    "    return accuracy\n",
    "\n",
    "# Cross entropy loss, including softmax activation.\n",
    "def compute_CE_loss(predictions, labels):\n",
    "    nr_instances = labels.shape[0]\n",
    "    loss = np.sum(-np.log(predictions[range(nr_instances),labels])) / nr_instances\n",
    "    return loss\n",
    "\n",
    "def create_batches(X, labels, batch_size):\n",
    "    \n",
    "    # Number of instances must be divisable by batch size.\n",
    "    assert X.shape[0]%batch_size == 0\n",
    "\n",
    "    list_with_batches = []\n",
    "    list_with_batchlabels = []\n",
    "    index = 0\n",
    "    for batch_nr in range(int(X.shape[0]/batch_size)):\n",
    "        start = index\n",
    "        end = index+batch_size\n",
    "        list_with_batches.append(X[start:end])\n",
    "        list_with_batchlabels.append(labels[start:end])\n",
    "\n",
    "        index += batch_size\n",
    "        \n",
    "    return list_with_batches, list_with_batchlabels\n",
    "\n",
    "# Do one forward pass and one backward pass.\n",
    "def learn(X, labels, action_per_layer, weights, biases, learning_rate = 0.1):\n",
    "    # Go through network in forward pass.\n",
    "    passed_network = forward_pass(X, action_per_layer, weights, biases)\n",
    "    # Update parameters.\n",
    "    weights, biases = backward_pass(passed_network, X, labels, action_per_layer,\n",
    "                                weights, biases, learning_rate = learning_rate)\n",
    "    predictions = do_softmax(passed_network[-1])\n",
    "    return predictions, weights, biases\n",
    "\n",
    "# Split data into batches and perform weight updates per batch.\n",
    "def learn_per_batch(X, labels, batch_size, action_per_layer, weights, biases, learning_rate):\n",
    "    \n",
    "    # Empty logs.\n",
    "    batch_acc_log = []\n",
    "    batch_loss_log = []\n",
    "    \n",
    "    # Split dataset up into batches of desired size.\n",
    "    batches, batchlabels = create_batches(X, labels, batch_size)\n",
    "\n",
    "    # Train and update for each batch.\n",
    "    for i in range(len(batches)):\n",
    "        # Train and update.\n",
    "        predictions, weights, biases = learn(batches[i], batchlabels[i], action_per_layer, weights, biases, learning_rate)\n",
    "        # Add batch accuracy and loss to log.\n",
    "        batch_acc_log.append(compute_accuracy(predictions, batchlabels[i]))\n",
    "        batch_loss_log.append(compute_CE_loss(predictions, batchlabels[i]))\n",
    "\n",
    "    # Take the average of all batches.\n",
    "    accuracy = np.average(batch_acc_log)\n",
    "    loss = np.average(batch_loss_log)\n",
    "    \n",
    "    return predictions, weights, biases, accuracy, loss\n",
    "\n",
    "# Check number of correct predictions.\n",
    "def validate(X, labels, action_per_layer, weights, biases):\n",
    "    # Test validation set.\n",
    "    passed_val_network = forward_pass(X, action_per_layer, weights, biases)\n",
    "    val_predictions = do_softmax(passed_val_network[-1])\n",
    "\n",
    "    # Calculate objectives.\n",
    "    accuracy = compute_accuracy(val_predictions, labels)\n",
    "    loss = compute_CE_loss(val_predictions, labels)\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "# Train network and return performance.\n",
    "def train_and_report(X_train, labels_train, X_val, labels_val, batch_size, action_per_layer, weights,\n",
    "                     biases, learning_rate, epochs):\n",
    "    \n",
    "    accuracy, loss = validate(X_train, labels_train, action_per_layer, weights, biases)\n",
    "    accuracy_log = [accuracy]\n",
    "    loss_log = [loss]\n",
    "    \n",
    "    accuracy, loss = validate(X_val, labels_val, action_per_layer, weights, biases)\n",
    "    val_acc_log = [accuracy]\n",
    "    val_loss_log = [loss]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train and update.\n",
    "        predictions, weights, biases, accuracy, loss = learn_per_batch(X_train, labels_train,\n",
    "                                                                       batch_size, action_per_layer,\n",
    "                                                                       weights, biases, learning_rate)\n",
    "\n",
    "        # Store accuracy and loss.\n",
    "        accuracy_log.append(accuracy)\n",
    "        loss_log.append(loss)\n",
    "        \n",
    "        # Validate and store objectives.\n",
    "        val_acc, val_loss = validate(X_val, labels_val, action_per_layer, weights, biases)\n",
    "        val_loss_log.append(val_loss)\n",
    "        val_acc_log.append(val_acc)\n",
    "\n",
    "        print(\"Epoch\", epoch, \"done\")\n",
    "    print(\"DONE\")\n",
    "    \n",
    "    plot_learning_curve(accuracy_log, val_acc_log, loss_log, val_loss_log)\n",
    "    \n",
    "    return weights, biases, accuracy_log, val_acc_log, loss_log, val_loss_log\n",
    "\n",
    "# Create matplotlib figure for learning curve with accuracy and loss of training and validation sets.\n",
    "def plot_learning_curve(accuracy_log, val_acc_log, loss_log, val_loss_log):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n",
    "    axes[0].plot(accuracy_log, label='Training set', color = \"#008b8e\")\n",
    "    axes[0].plot(val_acc_log, label='Validation set', color = \"#fabf01\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].legend(loc='best') # place legend\n",
    "    axes[0].set_title(\"Accuracy over epochs\")\n",
    "    axes[0].grid(True) # add raster lines\n",
    "    axes[0].xaxis.set_major_locator(plt.MaxNLocator(integer=True)) # convert x axis labels to integer values\n",
    "    axes[1].plot(loss_log, label='Training set', color = \"#bd467e\")\n",
    "    axes[1].plot(val_loss_log, label='Validation set', color = \"#919302\")\n",
    "    axes[1].legend(loc='best')\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Loss over epochs\")\n",
    "    axes[1].grid(True) # add raster lines\n",
    "    axes[1].xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    fig.tight_layout()\n",
    "\n",
    "# Load MNIST data.\n",
    "(xtrain, ytrain), (xval, yval), num_cls = load_mnist()\n",
    "\n",
    "# Visualize some examples.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%ytrain[i])\n",
    "    plt.imshow(xtrain[i].reshape([28,28]),cmap='gray');\n",
    "    \n",
    "# Normalize\n",
    "norm_factor = 255\n",
    "xtrain = xtrain/norm_factor\n",
    "xval = xval/norm_factor\n",
    "\n",
    "print(\"--- Dimensions ---\")\n",
    "print(\"xtrain:\", np.shape(xtrain))\n",
    "print(\"ytrain:\", np.shape(ytrain))\n",
    "print(\"xval:\", np.shape(xval))\n",
    "print(\"yval\", np.shape(yval))\n",
    "print(\"\\n\")\n",
    "print(\"--- Value range ---\")\n",
    "print(\"xtrain max:\", xtrain.max(), \"min: \", xtrain.min(), \"mean:\", xtrain.mean(), \"variance:\", xtrain.var())\n",
    "print(\"xval max:\", xval.max(), \"min: \", xval.min(), \"mean:\", xval.mean(), \"variance:\", xval.var())\n",
    "\n",
    "# Initialize weights with random normally distributed values.\n",
    "weights = [np.random.normal(loc = 0.0, scale = 1.0, size = (xtrain.shape[1],300)), # from input to first layer\n",
    "           [], # none for sigmoid-activated layer\n",
    "          np.random.normal(loc = 0.0, scale = 1.0, size = (300,num_cls))] # from sigmoid-activated layer to output layer\n",
    "\n",
    "biases = [300*[0], # for first layer\n",
    "         [], # none for sigmoid-activated layer\n",
    "         10*[0]] # for output layer\n",
    "\n",
    "action_per_layer = [\"dense\", \"sigmoid\", \"dense\"]\n",
    "\n",
    "weights, biases, accuracy_log, val_acc_log, loss_log, val_loss_log = train_and_report(xtrain, ytrain,\n",
    "                                                                                      xval, yval,\n",
    "                                                                                      batch_size = 100,\n",
    "                 action_per_layer = action_per_layer, weights = weights, biases = biases,\n",
    "                 learning_rate = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef73bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
