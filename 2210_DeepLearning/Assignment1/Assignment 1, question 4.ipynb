{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad19554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dimensions ---\n",
      "xtrain: (60000, 2)\n",
      "ytrain: (60000,)\n",
      "xval: (10000, 2)\n",
      "yval (10000,)\n",
      "\n",
      "\n",
      "--- Value range ---\n",
      "xtrain max: 4.285855641221728 min:  -4.852117653180117 mean: 0.0029853645282486114 variance: 0.9947430915341501\n",
      "xval max: 3.825215846396713 min:  -3.5775068414685856 mean: 0.012517284874245806 variance: 1.0033800061321618\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m loss_log \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     weights, biases, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     loss_log\u001b[38;5;241m.\u001b[39mappend(losses)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, y, weights, biases)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Apply sigmoid activation on the first layer.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(network[\u001b[38;5;241m1\u001b[39m])):\n\u001b[0;32m--> 121\u001b[0m     network[\u001b[38;5;241m1\u001b[39m][i] \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Compute the linear output.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(network[\u001b[38;5;241m2\u001b[39m])):\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def load_synth(num_train=60_000, num_val=10_000, seed=0):\n",
    "    \"\"\"\n",
    "    Load some very basic synthetic data that should be easy to classify. Two features, so that we can plot the\n",
    "    decision boundary (which is an ellipse in the feature space).\n",
    "\n",
    "    :param num_train: Number of training instances\n",
    "    :param num_val: Number of test/validation instances\n",
    "    :param num_features: Number of features per instance\n",
    "\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data with 2 features as a numpy floating point array, and the corresponding classification labels as a numpy\n",
    "     integer array. The second contains the test/validation data in the same format. The last integer contains the\n",
    "     number of classes (this is always 2 for this function).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    THRESHOLD = 0.6\n",
    "    quad = np.asarray([[1, -0.05], [1, .4]])\n",
    "\n",
    "    ntotal = num_train + num_val\n",
    "\n",
    "    x = np.random.randn(ntotal, 2)\n",
    "\n",
    "    # compute the quadratic form\n",
    "    q = np.einsum('bf, fk, bk -> b', x, quad, x)\n",
    "    y = (q > THRESHOLD).astype(int)\n",
    "\n",
    "    return (x[:num_train, :], y[:num_train]), (x[num_train:, :], y[num_train:]), 2\n",
    "\n",
    "def normalize(X):\n",
    "    # Calculate the mean.\n",
    "    feature1 = 0\n",
    "    feature2 = 0\n",
    "    for i in range(len(xtrain)):\n",
    "        feature1 += xtrain[i][0]\n",
    "        feature2 += xtrain[i][1]\n",
    "    mean_f1 = feature1/len(xtrain)\n",
    "    mean_f2 = feature2/len(xtrain)\n",
    "\n",
    "    # Calculate the variance.\n",
    "    deviations_f1 = 0\n",
    "    deviations_f2 = 0\n",
    "\n",
    "    for i in range(len(xtrain)):\n",
    "        deviations_f1 += xtrain[i][0] - mean_f1\n",
    "        deviations_f2 += xtrain[i][1] - mean_f2\n",
    "\n",
    "    variance_f1 = deviations_f1/len(xtrain)\n",
    "    variance_f2 = deviations_f2/len(xtrain)\n",
    "\n",
    "    xnormalized = []\n",
    "\n",
    "    for i in range(len(xtrain)):\n",
    "        norm_f1 = (xtrain[i][0]-mean_f1)/variance_f1\n",
    "        norm_f2 = (xtrain[i][0]-mean_f2)/variance_f2\n",
    "        xnormalized.append([norm_f1, norm_f2])\n",
    "    \n",
    "    xnormalized = np.asarray(xnormalized)\n",
    "    \n",
    "    return xnormalized\n",
    "\n",
    "def crossentropy(predictions, label):\n",
    "    loss = 0\n",
    "    for i in range(len(predictions)):\n",
    "        loss -= label * math.log(\n",
    "            predictions[i]+1.7976931348623157e+308) - (1-label) * math.log(\n",
    "            1-predictions[i]+1.7976931348623157e+308)\n",
    "    return loss\n",
    "\n",
    "def softmax(X):\n",
    "    exps = []\n",
    "    probs = []\n",
    "\n",
    "    for element in X:\n",
    "        # To prevent OverflowError:\n",
    "        if (element >= 709):\n",
    "            element = 709\n",
    "        # Softmax.\n",
    "        exp_of_X = math.exp(element)\n",
    "        exps.append(exp_of_X)\n",
    "        expsum = sum(exps)\n",
    "    \n",
    "    for i in range(len(exps)):\n",
    "        probs.append(exps[i] / (expsum+1.7976931348623157e+308))\n",
    "        \n",
    "    return probs\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def train(X, y, weights, biases):\n",
    "    # Create empty network.\n",
    "    network = [[0.0, 0.0, 0.0], # First layer k: 3 nodes\n",
    "               [0.0, 0.0, 0.0], # Layer h: same, 3 nodes\n",
    "               [0.0, 0.0], # Layer o: 2 nodes\n",
    "               [0.0, 0.0]] # Layer y: same, 2 nodes\n",
    "\n",
    "    deriv_network = [[0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0, 0.0],\n",
    "                     [0.0, 0.0],\n",
    "                     [0.0, 0.0]]\n",
    "    \n",
    "    loss_log = []\n",
    "    \n",
    "    for instance in range(len(xtrain)):\n",
    "        # Compute the first linear layer.\n",
    "        for j in range(len(network[0])):\n",
    "            for i in range(len(xtrain[instance])):\n",
    "                network[0][j] += weights[0][i][j] * xtrain[instance][i]\n",
    "                network[0][j] += biases[0][j]\n",
    "\n",
    "        # Apply sigmoid activation on the first layer.\n",
    "        for i in range(len(network[1])):\n",
    "            network[1][i] = sigmoid(network[1-1][i])\n",
    "\n",
    "        # Compute the linear output.\n",
    "        for j in range(len(network[2])):\n",
    "            for i in range(len(network[2-1])):\n",
    "                network[2][j] += weights[2][i][j] * network[2-1][i]\n",
    "                network[2][j] += biases[2][j]\n",
    "\n",
    "        # Apply softmax activation on the output.\n",
    "        network[3] = softmax(network[2])\n",
    "\n",
    "        loss = crossentropy(network[3], ytrain[instance])\n",
    "\n",
    "        loss_log.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        # Derivative of loss+softmax.\n",
    "        for i in range(len(network[2])):\n",
    "            deriv_network[2][i] = network[2][i]-ytrain[instance]\n",
    "\n",
    "        # Derivative of output layer to first layer, update weights and biases.\n",
    "        for j in range(len(network[2])):\n",
    "            for i in range(len(network[1])):\n",
    "                weights[2][i][j] = deriv_network[2][j] * network[1][i]\n",
    "                deriv_network[1][i] = deriv_network[2][j] * sum(weights[2][i])\n",
    "            biases[2][j] = deriv_network[2][j]\n",
    "\n",
    "        # Derivative of sigmoid layer.\n",
    "        for i in range(len(network[1])):\n",
    "            deriv_network[0][i] = deriv_network[1][i] * network[1][i] * (1-network[1][i])\n",
    "\n",
    "        # Derivative of first layer to input layer, update weights and biases.\n",
    "        for j in range(len(network[0])):\n",
    "            for i in range(len(xtrain[instance])):\n",
    "                weights[0][i][j] = deriv_network[0][j] * xtrain[instance][i]\n",
    "            biases[0][j] = deriv_network[0][j]\n",
    "\n",
    "    return weights, biases, loss_log\n",
    "\n",
    "# Load data\n",
    "(xtrain, ytrain), (xval, yval), num_cls = load_synth()\n",
    "\n",
    "print(\"--- Dimensions ---\")\n",
    "print(\"xtrain:\", np.shape(xtrain))\n",
    "print(\"ytrain:\", np.shape(ytrain))\n",
    "print(\"xval:\", np.shape(xval))\n",
    "print(\"yval\", np.shape(yval))\n",
    "print(\"\\n\")\n",
    "print(\"--- Value range ---\")\n",
    "print(\"xtrain max:\", xtrain.max(), \"min: \", xtrain.min(), \"mean:\",\n",
    "      xtrain.mean(), \"variance:\", xtrain.var())\n",
    "print(\"xval max:\", xval.max(), \"min: \", xval.min(), \"mean:\",\n",
    "      xval.mean(), \"variance:\", xval.var())\n",
    "\n",
    "xtrain = normalize(xtrain)\n",
    "xval = normalize(xval)\n",
    "\n",
    "# Initialize weights with random normally distributed values.\n",
    "weights = [np.random.normal(loc = 0.0, scale = 1.0,\n",
    "                            size = (xtrain.shape[1],3)), # from input to first layer\n",
    "           [], # none for sigmoid-activated layer\n",
    "          np.random.normal(loc = 0.0, scale = 1.0,\n",
    "                           size = (3,num_cls)), # from sigmoid-activated layer to output layer\n",
    "          []] # non for softmax-activated layer\n",
    "\n",
    "biases = [3*[0], # for first layer\n",
    "         [], # none for sigmoid-activated layer\n",
    "         num_cls*[0], # for output layer\n",
    "         []] # non for softmax-activated layer\n",
    "\n",
    "loss_log = []\n",
    "for epoch in range(5):\n",
    "    weights, biases, losses = train(xtrain, ytrain, weights, biases)\n",
    "    loss_log.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24415d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
